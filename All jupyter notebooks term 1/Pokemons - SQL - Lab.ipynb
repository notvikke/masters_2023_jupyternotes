{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mfLK8eBxPjU"
   },
   "source": [
    "<img width=\"200\" style=\"float:left\" \n",
    "     src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiRS6do3xPjb"
   },
   "source": [
    "# Sections\n",
    "* [Description](#0)\n",
    "* [1. Setup](#1)\n",
    "  * [1.1 Start Hadoop](#1.1)  \n",
    "  * [1.2 Search for Spark Installation](#1.2)\n",
    "  * [1.3 Create SparkSession](#1.3)\n",
    "* [2. Lab](#2)\n",
    "* [3. TearDown](#3)\n",
    "  * [3.1 Stop Hadoop](#3.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQZJ5NyOxPjd"
   },
   "source": [
    "<a id='0'></a>\n",
    "## Description\n",
    "<p>\n",
    "<div>The goals for this lab are:</div>\n",
    "<ul>    \n",
    "    <li>Get familiar with Spark SQL API</li>\n",
    "    <li>Apply some transformations using Spark SQL API</li>\n",
    "</ul>    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qprimq4axPje"
   },
   "source": [
    "<a id='1'></a>\n",
    "## 1. Setup\n",
    "\n",
    "Since we are going to process data stored from HDFS let's start the service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUIrQZS4xPjf"
   },
   "source": [
    "<a id='1.1'></a>\n",
    "### 1.1 Start Hadoop\n",
    "\n",
    "Start Hadoop\n",
    "\n",
    "Open a terminal and execute\n",
    "```sh\n",
    "hadoop-start.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76-Gbti4xPjf"
   },
   "source": [
    "<a id='1.2'></a>\n",
    "### 1.2 Search for Spark Installation \n",
    "This step is required just because we are working in the course environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CR1HycjxPjg"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILWPSokcxPjh"
   },
   "source": [
    "I'm changing pandas max column width property to improve data displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_9jqy9RxPji"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0P8ACFRixPjj"
   },
   "source": [
    "<a id='1.3'></a>\n",
    "### 1.3 Create SparkSession\n",
    "By setting this environment variable we can include extra libraries in our Spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4jjApyKxPjj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = ' pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muzArDgJxPjk"
   },
   "source": [
    "The first thing always is to create the SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    ".appName(\"Pokemons - SQL - Lab.ipynb\")\n",
    ".config(\"spark.sql.warehouse.dir\",\"hdfs://localhost:9000/warehouse\")\n",
    ".getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now the metastore should be empty (no tables) but just one 'default' database\n",
    "\n",
    "Also check the HDFS path, it should be empty as well.\n",
    "\n",
    "http://localhost:50070/explorer.html#/warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lists all the databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show databases\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows the current database in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select current_database()\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lists all tables in the current database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating managed tables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a managed table called pokemons.\n",
    "\n",
    "As we don't specify database it will belong to the 'default' database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemons = spark.read.parquet(\"hdfs://localhost:9000/datalake/std/pokemon/pokemon-data/\")\n",
    "spark.sql(\"drop table if exists pokemons\")\n",
    "pokemons.write.mode(\"overwrite\").saveAsTable(\"pokemons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check HDFS directory again\n",
    "\n",
    "http://localhost:50070/explorer.html#/warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there is a pokemon folder with files whithin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a managed table called sightings.\n",
    "\n",
    "As we don't specify database it will belong to the 'default' database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sightings = spark.read.parquet(\"hdfs://localhost:9000/datalake/raw/pokemon/pokemon-sightings/\")\n",
    "spark.sql(\"drop table if exists sightings\")\n",
    "sightings.write.mode(\"overwrite\").saveAsTable(\"sightings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the information in the metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we drop a managed table we will delete it from the metastore and from the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table pokemons\")\n",
    "spark.sql(\"drop table sightings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check HDFS directory, it should be empty again\n",
    "\n",
    "http://localhost:50070/explorer.html#/warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Creating external tables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case I want to organize the tables in a custom database rather than the 'default' one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"create database if not exists pokemons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show databases\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select current_database()\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"use pokemons\")\n",
    "spark.sql(\"select current_database()\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create an external tables from data that is already stored in HDFS\n",
    "\n",
    "This is the preferred way of working with tables in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "create table pokemons.pokemons\n",
    "using parquet\n",
    "location 'hdfs://localhost:9000/datalake/std/pokemon/pokemon-data/'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from pokemons.pokemons\").limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "create table pokemons.sightings\n",
    "using parquet\n",
    "location 'hdfs://localhost:9000/datalake/raw/pokemon/pokemon-sightings/'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from pokemons.sightings\").limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the information in the metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we drop external tables we will delete it from the metastore but data will remain int the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table if exists pokemons.pokemons\")\n",
    "spark.sql(\"drop table if exists pokemons.sightings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check HDFS directories, the still have the files\n",
    "\n",
    "http://localhost:50070/explorer.html#/datalake/std/pokemon/pokemon-data/\n",
    "\n",
    "http://localhost:50070/explorer.html#/datalake/raw/pokemon/pokemon-sightings/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets' create them again to keep working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "create table pokemons.pokemons\n",
    "using parquet\n",
    "location 'hdfs://localhost:9000/datalake/std/pokemon/pokemon-data/'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "create table pokemons.sightings\n",
    "using parquet\n",
    "location 'hdfs://localhost:9000/datalake/raw/pokemon/pokemon-sightings/'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 DataFrame Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all legendary pokemons from generations 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from pokemons.pokemons where generation in (1,2) and legendary order by total desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column with emojis based on type_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "            select\n",
    "                *,\n",
    "                case \n",
    "                    when type_1='Water' then '💧'\n",
    "                    when type_1='Fire' then '🔥'\n",
    "                    when type_1='Electric' then '⚡️'\n",
    "                    when type_1='Ice' then '❄️'\n",
    "                    when type_1='Grass' then '🌿'\n",
    "                    when type_1='Dragon' then '🐉'\n",
    "                    when type_1='Psychic' then '🧠'\n",
    "                    when type_1='Ghost' then '👻'\n",
    "                    when type_1='Bug' then '🐛'\n",
    "                    when type_1='Poison' then '☠️'\n",
    "                end as emoji\n",
    "                from pokemons.pokemons\n",
    "          \"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same thing by creating our custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "def emoji(str):\n",
    "    res = None\n",
    "    if str==\"Water\":\n",
    "        res = \"💧\"\n",
    "    elif str==\"Fire\":\n",
    "        res = \"🔥\"\n",
    "    elif str==\"Electric\":\n",
    "        res = \"⚡️\"\n",
    "    elif str==\"Ice\":\n",
    "        res = \"❄️\"\n",
    "    elif str==\"Grass\":\n",
    "        res = \"🌿\"\n",
    "    elif str==\"Dragon\":\n",
    "        res = \"🐉\"\n",
    "    elif str==\"Psychic\":\n",
    "        res = \"🧠\"\n",
    "    elif str==\"Ghost\":\n",
    "        res = \"👻\"\n",
    "    elif str==\"Bug\":\n",
    "        res = \"🐛\"\n",
    "    elif str==\"Poison\":\n",
    "        res = \"☠️\"\n",
    "    return res\n",
    "\n",
    "spark.udf.register(\"emoji_sql\", emoji, StringType())\n",
    "\n",
    "spark.sql(\"select *, emoji_sql(type_1) as emoji from pokemons.pokemons\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a three columns today,tomorrow and now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "           select *,\n",
    "             current_date() as today,\n",
    "             date_add(current_date(),1) as tomorrow,\n",
    "             current_timestamp() as now\n",
    "           from pokemons.pokemons\n",
    "          \"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practicing date/time functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate basic stats per pokemon type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "            select\n",
    "            type_1,\n",
    "            count(*) as count,\n",
    "            max(hp) as max_hp,\n",
    "            round(avg(hp),2) as avg_hp,\n",
    "            min(hp) as min_hp\n",
    "            from pokemons.pokemons\n",
    "            group by type_1\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join pokemons and sightings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all pokemon sightings in Spain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_spain(longitude,latitude):\n",
    "    # Spain bounding box (aka bbox)\n",
    "    return (longitude > -18.3936845) & (longitude < 4.5918885) & (latitude > 27.4335426) & (latitude  < 43.9933088) \n",
    "\n",
    "spark.udf.register(\"in_spain_sql\", in_spain, BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemons_in_spain = spark.sql(\"\"\"\n",
    "                                select\n",
    "                                id, name, total, hp, \n",
    "                                location.coordinates[0] as longitude,\n",
    "                                location.coordinates[1] as latitude\n",
    "                                from pokemons.pokemons l join pokemons.sightings r\n",
    "                                on l.id=r.pokemonId\n",
    "                                where in_spain_sql(location.coordinates[0],location.coordinates[1])\n",
    "                                  \"\"\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemons_in_spain.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4  Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show user functions\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "spark.sql(\"show system functions\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Tear Down\n",
    "\n",
    "Once we complete the the lab we can stop all the services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.1'></a>\n",
    "### 3.1 Stop Hadoop\n",
    "\n",
    "Stops Hadoop\n",
    "Open a terminal and execute\n",
    "```sh\n",
    "hadoop-stop.sh\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
