{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python36","display_name":"Python 3.6","language":"python"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","name":"python","pygments_lexer":"ipython3","version":"3.6.6","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"}},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"e949625e2e5a471aa41e4f92b08c31ba":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_70319b3898694c70a46337b895c5bd6e","IPY_MODEL_4c56d7b686c44844be150b295ca1c76d","IPY_MODEL_b58fd8844d404b178c80c890bfd4995b"],"layout":"IPY_MODEL_e1aef73f171143dab4656b7ee7b662ee"}},"70319b3898694c70a46337b895c5bd6e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d28e40810ca4d21adcc5992361b0b71","placeholder":"​","style":"IPY_MODEL_cb28fa6fb2404307a8dae34c726e68c2","value":"100%"}},"4c56d7b686c44844be150b295ca1c76d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_54cb4486acc942d99f12c86312b809f4","max":46830571,"min":0,"orientation":"horizontal","style":"IPY_MODEL_185fbe356554494699a2ef6f20d1cc13","value":46830571}},"b58fd8844d404b178c80c890bfd4995b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ca7f64b9a53428888adf74e51fba6ad","placeholder":"​","style":"IPY_MODEL_dd286e1b90f443b2b382c1d6f47ea01b","value":" 44.7M/44.7M [00:00&lt;00:00, 117MB/s]"}},"e1aef73f171143dab4656b7ee7b662ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d28e40810ca4d21adcc5992361b0b71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb28fa6fb2404307a8dae34c726e68c2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"54cb4486acc942d99f12c86312b809f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"185fbe356554494699a2ef6f20d1cc13":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0ca7f64b9a53428888adf74e51fba6ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd286e1b90f443b2b382c1d6f47ea01b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"sMe9jO95wqay"},"source":["# Transfer Learning\n","A Convolutional Neural Network (CNN) for image classification is made up of multiple layers that extract features, such as edges, corners, etc; and then use a final fully-connected layer to classify objects based on these features. You can visualize this like this:\n","\n","<table>\n","    <tr><td rowspan=2 style='border: 1px solid black;'>&#x21d2;</td><td style='border: 1px solid black;'>Convolutional Layer</td><td style='border: 1px solid black;'>Pooling Layer</td><td style='border: 1px solid black;'>Convolutional Layer</td><td style='border: 1px solid black;'>Pooling Layer</td><td style='border: 1px solid black;'>Fully Connected Layer</td><td rowspan=2 style='border: 1px solid black;'>&#x21d2;</td></tr>\n","    <tr><td colspan=4 style='border: 1px solid black; text-align:center;'>Feature Extraction</td><td style='border: 1px solid black; text-align:center;'>Classification</td></tr>\n","</table>\n","\n","*Transfer Learning* is a technique where you can take an existing trained model and re-use its feature extraction layers, replacing its final classification layer with a fully-connected layer trained on your own custom images. With this technique, your model benefits from the feature extraction training that was performed on the base model (which may have been based on a larger training dataset than you have access to) to build a classification model for your own specific set of object classes.\n","\n","How does this help? Well, think of it this way. Suppose you take a professional tennis player and a complete beginner, and try to teach them both how to play raquetball. It's reasonable to assume that the professional tennis player will be easier to train, because many of the underlying skills involved in raquetball are already learned. Similarly, a pre-trained CNN model may be easier to train to classify specific set of objects because it's already learned how to identify the features of common objects, such as edges and corners. Fundamentally, a pre-trained model can be a great way to produce an effective classifier even when you have limited data with which to train it.\n","\n","In this notebook, we'll see how to implement transfer learning for a classification model using PyTorch.\n","\n","> **Important**:The base model used in this exercise is large, and training is resource-intensive. Before running the code in this notebook, shut down all other notebooks in this library (In each open notebook other than this one, on the **File** menu, click **Close and Halt**). If you experience and Out-of-Memory (OOM) error when running code in this notebook, shut down this entire library, and then reopen it and open only this notebook.\n","\n","## Using Transfer Learning to Train a CNN\n","\n","First, we'll import the latest version of PyTorch and prepare to load our training data.\n","\n","> *Note: The following `pip install` commands install the CPU-based version of PyTorch on Linux, which is appropriate for the Azure Notebooks environment. For instructions on how to install the PyTorch and TorchVision packages on your own system, see https://pytorch.org/get-started/locally/*"]},{"cell_type":"code","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"puaeNNMfwqbF","executionInfo":{"status":"ok","timestamp":1671093559316,"user_tz":-60,"elapsed":6625,"user":{"displayName":"Hind Azegrouz","userId":"04469582049561885297"}},"outputId":"c75de32d-b5ed-45c7-f30e-0db02a0fb90e"},"source":["\n","\n","# Import PyTorch libraries\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import os\n","\n","print(\"Libraries imported - ready to use PyTorch\", torch.__version__)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Libraries imported - ready to use PyTorch 1.13.0+cu116\n"]}]},{"cell_type":"markdown","metadata":{"id":"TPqWG8KZwqbH"},"source":["### Load and Prepare the Data\n","Before we can train the model to classify images based on our shape classes, we need to prepare the training data. PyTorch includes functions for loading and transforming data. We'll use these to create an iterative loader for training data, and a second iterative loader for test data (which we'll use to validate the trained model). The loaders will transform the image data to match the format used to train the original resnet CNN model, and finally convert the image data into *tensors*, which are the core data structure used in PyTorch.\n","\n","Run the following cell to define the data loaders, and then load the first batch of 32 training images and display them along with their class labels."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BA8XM7Vxw0D2","executionInfo":{"status":"ok","timestamp":1671093580633,"user_tz":-60,"elapsed":21333,"user":{"displayName":"Hind Azegrouz","userId":"04469582049561885297"}},"outputId":"6af906fb-1f6c-4167-f8b9-110a965604d4"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"F4CSkw8dwqbH","executionInfo":{"status":"ok","timestamp":1671095108822,"user_tz":-60,"elapsed":4090,"user":{"displayName":"Hind Azegrouz","userId":"04469582049561885297"}},"outputId":"98eb8f3b-cbcb-4379-a995-0baa9c98446b"},"source":["# Function to ingest data using training and test loaders\n","def load_dataset(data_path):\n","    \n","    # Resize to 256 x 256, center-crop to 224x224 (to match the resnet image size), and convert to Tensor\n","    transformation = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","    ])\n","\n","    # Load all of the images, transforming them\n","    full_dataset = torchvision.datasets.ImageFolder(\n","        root=data_path,\n","        transform=transformation\n","    )\n","    \n","    # Split into training (70%) and testing (30%) datasets)\n","    train_size = int(0.7 * len(full_dataset))\n","    test_size = len(full_dataset) - train_size\n","    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n","    \n","    # define a loader for the training data we can iterate through in 32-image batches\n","    train_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=15,\n","        num_workers=0,\n","        shuffle=False\n","    )\n","    \n","    # define a loader for the testing data we can iterate through in 32-image batches\n","    test_loader = torch.utils.data.DataLoader(\n","        test_dataset,\n","        batch_size=15,\n","        num_workers=0,\n","        shuffle=False\n","    )\n","        \n","    return train_loader, test_loader\n","\n","\n","import os \n","\n","# The images are in a folder named 'shapes/training'\n","training_folder_name = '/content/gdrive/My Drive/IE/Notebooks_s4/shapes/training'\n","\n","# The folder contains a subfolder for each class of shape\n","classes = sorted(os.listdir(training_folder_name))\n","print(classes)\n","\n","# Get the iterative dataloaders for test and training data\n","train_loader, test_loader = load_dataset(training_folder_name)\n","print(\"Data loaders ready to read\", training_folder_name)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["['circle', 'square', 'triangle']\n","Data loaders ready to read /content/gdrive/My Drive/IE/Notebooks_s4/shapes/training\n"]}]},{"cell_type":"markdown","metadata":{"id":"THpvMZ6ewqbJ"},"source":["### Download a trained model to user as a base\n","The ***resnet*** model is an CNN-based image classifier that has been pre-trained using a huge dataset containing thousands of images of many kinds of object. We'll download the trained model, excluding its final linear layer, and freeze the convolutional layers to retain the trained weights. Then we'll add a new linear layer that will map the features extracted by the convolutional layers to the classes of our shape images."]},{"cell_type":"code","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["e949625e2e5a471aa41e4f92b08c31ba","70319b3898694c70a46337b895c5bd6e","4c56d7b686c44844be150b295ca1c76d","b58fd8844d404b178c80c890bfd4995b","e1aef73f171143dab4656b7ee7b662ee","1d28e40810ca4d21adcc5992361b0b71","cb28fa6fb2404307a8dae34c726e68c2","54cb4486acc942d99f12c86312b809f4","185fbe356554494699a2ef6f20d1cc13","0ca7f64b9a53428888adf74e51fba6ad","dd286e1b90f443b2b382c1d6f47ea01b"]},"id":"tyLx1GWXwqbK","executionInfo":{"status":"ok","timestamp":1671093585138,"user_tz":-60,"elapsed":1209,"user":{"displayName":"Hind Azegrouz","userId":"04469582049561885297"}},"outputId":"4e8fc8f4-01a3-4225-cb9b-0ed9722854bf"},"source":["model_resnet = torchvision.models.resnet18(pretrained=True)\n","for param in model_resnet.parameters():\n","    param.requires_grad = False\n","\n","num_ftrs = model_resnet.fc.in_features\n","model_resnet.fc = nn.Linear(num_ftrs, len(classes))\n","\n","# Now print the full model, which will include the layers of the base model plus the linear layer we added\n","print(model_resnet)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/44.7M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e949625e2e5a471aa41e4f92b08c31ba"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=3, bias=True)\n",")\n"]}]},{"cell_type":"markdown","metadata":{"id":"g493UQdfwqbL"},"source":["### Train the Model\n","With the layers of the CNN defined, we're ready to train the top layer using our image data. This will take a considerable amount of time on a CPU due to the complexity of the base model, so we'll train the model over only one epoch."]},{"cell_type":"code","metadata":{"scrolled":false,"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"tNaGTYazwqbO","executionInfo":{"status":"ok","timestamp":1671093866631,"user_tz":-60,"elapsed":281496,"user":{"displayName":"Hind Azegrouz","userId":"04469582049561885297"}},"outputId":"c7f831d5-9bf3-4035-c2f5-16211b013c6f"},"source":["def train(model, device, train_loader, optimizer, epoch):\n","    # Set the model to training mode\n","    model.train()\n","    train_loss = 0\n","    print(\"Epoch:\", epoch)\n","    # Process the images in batches\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        # Use the CPU or GPU as appropriate\n","        data, target = data.to(device), target.to(device)\n","        \n","        # Reset the optimizer\n","        optimizer.zero_grad()\n","        \n","        # Push the data forward through the model layers\n","        output = model(data)\n","        \n","        # Get the loss\n","        loss = loss_criteria(output, target)\n","\n","        # Keep a running total\n","        train_loss += loss.item()\n","        \n","        # Backpropagate\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # Print metrics so we see some progress\n","        print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n","            \n","    # return average loss for the epoch\n","    avg_loss = train_loss / batch_idx+1\n","    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n","    return avg_loss\n","            \n","            \n","def test(model, device, test_loader):\n","    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        batch_count = 0\n","        for data, target in test_loader:\n","            batch_count += 1\n","            data, target = data.to(device), target.to(device)\n","            \n","            # Get the predicted classes for this batch\n","            output = model(data)\n","            \n","            # Calculate the loss for this batch\n","            test_loss += loss_criteria(output, target).item()\n","            \n","            # Calculate the accuracy for this batch\n","            _, predicted = torch.max(output.data, 1)\n","            correct += torch.sum(target==predicted).item()\n","\n","    # Calculate the average loss and total accuracy for this epoch\n","    avg_loss = test_loss/batch_count\n","    print('Validation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        avg_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","    \n","    # return average loss for the epoch\n","    return avg_loss\n","    \n","# Now use the train and test functions to train and test the model    \n","\n","device = \"cpu\"\n","if (torch.cuda.is_available()):\n","    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n","    device = \"cuda\"\n","print('Training on', device)\n","\n","# Create an instance of the model class and allocate it to the device\n","model_resnet = model_resnet.to(device)\n","\n","# Use an \"Adam\" optimizer to adjust weights\n","# (see https://pytorch.org/docs/stable/optim.html#algorithms for details of supported algorithms)\n","optimizer = optim.Adam(model_resnet.parameters(), lr=0.001)\n","\n","# Specify the loss criteria\n","loss_criteria = nn.CrossEntropyLoss()\n","\n","# Track metrics in these arrays\n","epoch_nums = []\n","training_loss = []\n","validation_loss = []\n","\n","# Train over 1 epochs\n","epochs = 1\n","for epoch in range(1, epochs + 1):\n","        train_loss = train(model_resnet, device, train_loader, optimizer, epoch)\n","        test_loss = test(model_resnet, device, test_loader)\n","        epoch_nums.append(epoch)\n","        training_loss.append(train_loss)\n","        validation_loss.append(test_loss)\n"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Training on cpu\n","Epoch: 1\n","\tTraining batch 1 Loss: 1.146975\n","\tTraining batch 2 Loss: 1.308035\n","\tTraining batch 3 Loss: 1.460398\n","\tTraining batch 4 Loss: 1.399942\n","\tTraining batch 5 Loss: 1.054751\n","\tTraining batch 6 Loss: 1.106237\n","\tTraining batch 7 Loss: 1.049929\n","\tTraining batch 8 Loss: 1.050015\n","\tTraining batch 9 Loss: 0.971117\n","\tTraining batch 10 Loss: 0.873964\n","\tTraining batch 11 Loss: 1.042102\n","\tTraining batch 12 Loss: 0.858401\n","\tTraining batch 13 Loss: 1.558802\n","\tTraining batch 14 Loss: 0.963903\n","\tTraining batch 15 Loss: 1.275704\n","\tTraining batch 16 Loss: 0.634900\n","\tTraining batch 17 Loss: 0.803968\n","\tTraining batch 18 Loss: 0.711576\n","\tTraining batch 19 Loss: 0.659247\n","\tTraining batch 20 Loss: 0.634498\n","\tTraining batch 21 Loss: 0.760872\n","\tTraining batch 22 Loss: 0.755187\n","\tTraining batch 23 Loss: 0.458384\n","\tTraining batch 24 Loss: 0.741684\n","\tTraining batch 25 Loss: 0.576852\n","\tTraining batch 26 Loss: 0.831829\n","\tTraining batch 27 Loss: 0.638966\n","\tTraining batch 28 Loss: 0.484200\n","\tTraining batch 29 Loss: 0.420141\n","\tTraining batch 30 Loss: 0.460437\n","\tTraining batch 31 Loss: 0.566290\n","\tTraining batch 32 Loss: 0.440392\n","\tTraining batch 33 Loss: 0.568376\n","\tTraining batch 34 Loss: 0.572990\n","\tTraining batch 35 Loss: 0.427599\n","\tTraining batch 36 Loss: 0.385075\n","\tTraining batch 37 Loss: 0.316836\n","\tTraining batch 38 Loss: 0.257930\n","\tTraining batch 39 Loss: 0.267788\n","\tTraining batch 40 Loss: 0.323371\n","\tTraining batch 41 Loss: 0.295000\n","\tTraining batch 42 Loss: 0.353889\n","\tTraining batch 43 Loss: 0.403667\n","\tTraining batch 44 Loss: 0.313384\n","\tTraining batch 45 Loss: 0.317466\n","\tTraining batch 46 Loss: 0.249954\n","\tTraining batch 47 Loss: 0.282150\n","\tTraining batch 48 Loss: 0.216915\n","\tTraining batch 49 Loss: 0.192604\n","\tTraining batch 50 Loss: 0.436906\n","\tTraining batch 51 Loss: 0.251079\n","\tTraining batch 52 Loss: 0.223094\n","\tTraining batch 53 Loss: 0.201093\n","\tTraining batch 54 Loss: 0.245270\n","\tTraining batch 55 Loss: 0.191121\n","\tTraining batch 56 Loss: 0.252913\n","Training set: Average loss: 1.640839\n","Validation set: Average loss: 0.1788, Accuracy: 359/360 (100%)\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"DFelJdfSwqbP"},"source":["### Using the Trained Model\n","Now that we've trained the model, we can use it to predict the class of an image."]},{"cell_type":"code","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"CKkU5GSdwqbQ","executionInfo":{"status":"ok","timestamp":1671093866632,"user_tz":-60,"elapsed":8,"user":{"displayName":"Hind Azegrouz","userId":"04469582049561885297"}},"outputId":"27750039-a19b-4965-9c50-25b7b763b8f9"},"source":["# Helper function to resize image\n","def resize_image(src_img, size=(128,128), bg_color=\"white\"): \n","    from PIL import Image\n","\n","    # rescale the image so the longest edge is the right size\n","    src_img.thumbnail(size, Image.ANTIALIAS)\n","    \n","    # Create a new image of the right shape\n","    new_image = Image.new(\"RGB\", size, bg_color)\n","    \n","    # Paste the rescaled image onto the new background\n","    new_image.paste(src_img, (int((size[0] - src_img.size[0]) / 2), int((size[1] - src_img.size[1]) / 2)))\n","  \n","    # return the resized image\n","    return new_image\n","\n","# Function to predict the class of an image\n","def predict_image(classifier, image_array):\n","   \n","    # Set the classifer model to evaluation mode\n","    classifier.eval()\n","    \n","    # These are the classes our model can predict\n","    class_names = ['circle', 'square', 'triangle']\n","    \n","    # Apply the same transformations as we did for the training images\n","    transformation = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","    ])\n","\n","    # Preprocess the imagees\n","    image_tensor = torch.stack([transformation(image).float() for image in image_array])\n","\n","    # Turn the input into a Variable\n","    input_features = image_tensor\n","\n","    # Predict the class of each input image\n","    predictions = classifier(input_features)\n","    \n","    predicted_classes = []\n","    # Convert the predictions to a numpy array \n","    for prediction in predictions.data.numpy():\n","        # The prediction for each image is the probability for each class, e.g. [0.8, 0.1, 0.2]\n","        # So get the index of the highest probability\n","        class_idx = np.argmax(prediction)\n","        # And append the corresponding class name to the results\n","        predicted_classes.append(class_names[class_idx])\n","    return np.array(predicted_classes)\n","\n","print(\"Functions created - ready to use model for inference.\")"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Functions created - ready to use model for inference.\n"]}]},{"cell_type":"code","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":160},"id":"1u0XlpxswqbS","executionInfo":{"status":"ok","timestamp":1671095133584,"user_tz":-60,"elapsed":2451,"user":{"displayName":"Hind Azegrouz","userId":"04469582049561885297"}},"outputId":"d09268b3-6b33-4183-d42c-332f0c9a2e74"},"source":["import os\n","from random import randint\n","import numpy as np\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","\n","model_resnet.to(\"cpu\")\n","\n","# Get the list of test image files\n","test_folder = '/content/gdrive/My Drive/IE/Notebooks_s4/shapes/test'\n","test_image_files = os.listdir(test_folder)\n","\n","# Empty array on which to store the images\n","image_arrays = []\n","\n","size = (224,224)\n","background_color=\"white\"\n","\n","fig = plt.figure(figsize=(12, 8))\n","\n","# Get the images and show the predicted classes\n","for file_idx in range(len(test_image_files)):\n","    img = Image.open(os.path.join(test_folder, test_image_files[file_idx]))\n","    \n","    # resize the image so it matches the training set - it  must be the same size as the images on which the model was trained\n","    resized_img = np.array(resize_image(img, size, background_color))\n","                      \n","    # Add the image to the array of images\n","    image_arrays.append(resized_img)\n","\n","# Get predictions from the array of image arrays\n","# Note that the model expects an array of 1 or more images - just like the batches on which it was trained\n","predictions = predict_image(model_resnet, np.array(image_arrays))\n","\n","# plot easch image with its corresponding prediction\n","for idx in range(len(predictions)):\n","    a=fig.add_subplot(1,len(predictions),idx+1)\n","    imgplot = plt.imshow(image_arrays[idx])\n","    a.set_title(predictions[idx])\n"],"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 864x576 with 6 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]}]}