{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUezTS_amIw1"
   },
   "source": [
    "### Hello everyone to the Session 9 about Hyperparameter Tuning and Pipelines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ae78pJNeEVdI",
    "tags": []
   },
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-BCV4VpIS4S"
   },
   "source": [
    "## What are these hyperparameters?\n",
    "  * Parameters are the components that are learned by the model during the modelling process. \n",
    "  * Hyperparameters are NOT learned during the process, but instead set before the modelling process. \n",
    "    * Some are not improving our model performance, while some are. \n",
    "    * Take for example n_jobs in *RandomForestClassifier*, that simply describes how many jobs to run in parallel. \n",
    "    * Whereas, *n_estimators* describes the count of trees in the forest or *max_features* on how many features to split on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ndtJXReNAuG"
   },
   "source": [
    "## Exercise 1 - Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SH719NcWmKM"
   },
   "source": [
    "### Exercise 1.0 - Import the telco dataset, split it to features and labels and training and test sets. \n",
    "\n",
    "* Import the telco churn dataset from Session 7. \n",
    "* Split the dataset between target and features. \n",
    "* Use 80/20 train_test_split with random_seed of 99."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is also available below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KUlgZwgiNPkQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import the data\n",
    "churn_df = pd.read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vT7M4x2vLWiCYW9YrLrLxPQWj0XAG8h71lGMLfUJvzH1qsVR-fqpGYl53Luvi_B1xBe8qw1mKos-tFw/pub?output=csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas_profiling\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score,recall_score,precision_score,f1_score\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.0.1 - Create the target and features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = churn_df.drop(\"Churn\", axis=1)\n",
    "y=churn_df[\"Churn\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.0.2 - Create test train split with 80/20 split, random seed of 99 and use stratification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exericse 1.0.3 - Import the StandardScaler from preprocessing library and instantiate the StandardScaler object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.0.4 - Using StandardScaler, fit-transform the features training set and transform the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scl.fit_transform(X_train)\n",
    "X_test = scl.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNjnHYFTNFcn"
   },
   "source": [
    "### Exercise 1.1 - Hyperparameter tuning with for loop for KNeighborsClassifier. \n",
    "* Build estimators for 10, 20 and 30 K-neighbors. [Docs](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.kneighbors)\n",
    "* Fit each of the three estimators to training data.\n",
    "* Create predictions of the fitted data. \n",
    "* Get accuracy score for each of the models.\n",
    "- PS! Use loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbours =  10 acc =  0.9523809523809523\n",
      "neighbours =  20 acc =  0.9349206349206349\n",
      "neighbours =  30 acc =  0.9301587301587302\n"
     ]
    }
   ],
   "source": [
    "for n in range(10,31,10):\n",
    "    knn = KNeighborsClassifier(n_neighbors=n)\n",
    "    knn_fit= knn.fit(X_train, y_train)\n",
    "    knn_pred= knn_fit.predict(X_test)\n",
    "    knn_acc = accuracy_score(y_test,knn_pred)\n",
    "    print(\"neighbours = \", n, \"acc = \", knn_acc)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRDiWbICVh8J"
   },
   "source": [
    "#### 1.1.1 - The programmatic way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7SQinBUjV5-"
   },
   "source": [
    "#### 1.2 Now repeat the process using RandomForestClassifier. [Docs](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)\n",
    "- Use a combination of n_estimators of 300, 500, 800 and max_depth of 8, 10, 12, 14.\n",
    "- How many models did we create? Count each of the iteration when using a for loop and print the total count. \n",
    "- Assign these values into a dataframe of *df_rf_accs*. Use columns *n_estimators, max_depth, accuracy*.\n",
    "- What are the top three combinations from *df_rf_accs*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "rf_accs=[]\n",
    "c=0\n",
    "\n",
    "for n_estimator in [300,500,800]:\n",
    "    for depth in range(8,15,2):\n",
    "        rf = RandomForestClassifier(n_estimators = n_estimator, max_depth = depth, random_state= 97)\n",
    "        rf_fit = rf.fit(X_train, y_train)\n",
    "        rf_pred = rf_fit.predict(X_test)\n",
    "        rf_accs.append([n_estimator,depth,accuracy_score(y_test,rf_pred)])\n",
    "        c+=1\n",
    "        \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJP_25MHo3Ek"
   },
   "source": [
    "### 1.3 - Now repeat the process using GradientBoostingClassifier [Docs](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)\n",
    "- Use 5 learning rates evenly spread between 0.01 and 1. \n",
    "- Use crossvalidation with 5 folds. \n",
    "- Round these values to 4th decimal. \n",
    "- Print out each learning rate with it's current iteration number, accuracies, mean accuracy and standard deviation.\n",
    "\n",
    "#### How many times did we run the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbc_score =  [0.89880952 0.9047619  0.91269841 0.89880952 0.8968254 ]\n",
      "gbc_score =  [0.95238095 0.95039683 0.96031746 0.94444444 0.93452381]\n",
      "gbc_score =  [0.94642857 0.95039683 0.96031746 0.95238095 0.94047619]\n",
      "gbc_score =  [0.93849206 0.9484127  0.95833333 0.94246032 0.93253968]\n",
      "gbc_score =  [0.95833333 0.95039683 0.93849206 0.93452381 0.92857143]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "for lr in np.linspace(.01,1,5):\n",
    "    rounded_lr = lr.round(4)\n",
    "    gbc = GradientBoostingClassifier(learning_rate = rounded_lr, random_state =97)\n",
    "    gbc_cv_score = cross_val_score(gbc, X_train, y_train, cv=5, scoring =\"accuracy\")\n",
    "    \n",
    "    print( \"gbc_score = \",gbc_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gbc_base = GradientBoostingClassifier(random_state =97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6QsH_H641Yj"
   },
   "source": [
    "### 1.4 - Now repeat the previous process using GradientBoostingClassifier together with Sklearns Grid Search.\n",
    "* Use 1/2 of your available cores to parallelize the execution. \n",
    "* Use GradientBoostingClassifier as the estimator with random state of 77. \n",
    "* Use the following hyperparameters (params): \n",
    "    * Five learning rates between 0.2575 and 0.7525. \n",
    "    * Max depth of 2, 4, 6.\n",
    "    * n-estimators of 120, 200, 280.\n",
    "* Use 5 k-folds for cross-validation. \n",
    "* Use accuracy for scoring. \n",
    "* Refit the best hyperparameters.\n",
    "\n",
    "PS. If you need help then refer back to your 10th Python class session\n",
    "\n",
    "#### Use GridSearch properties to return it's log statistics. \n",
    "* What are the `best params`? \n",
    "* What is the `best score`? \n",
    "* What is the log statistics of the top five performers?\n",
    "* How long does it take for the Grid Search?\n",
    "* Given the range we used, did we find the global optimal hyperparameters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate' : [*np.linspace(.2575, .7525, 5).round(4)],\n",
    "    'max_depth' : [*range(2,7,2)],\n",
    "    'n_estimators' : list(range(150,451,150))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_s = GridSearchCV(\n",
    "    estimator = gbc_base,\n",
    "    param_grid = params,\n",
    "    scoring = \"accuracy\",\n",
    "    n_jobs = int((os.cpu_count()/2)),\n",
    "    cv=5,\n",
    "    refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GradientBoostingClassifier(random_state=97),\n",
       "             n_jobs=6,\n",
       "             param_grid={'learning_rate': [0.2575, 0.3812, 0.505, 0.6287,\n",
       "                                           0.7525],\n",
       "                         'max_depth': [2, 4, 6],\n",
       "                         'n_estimators': [150, 300, 450]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_s.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgrid_s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_params_\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5569\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   5570\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   5571\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   5572\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5573\u001b[0m ):\n\u001b[0;32m   5574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 5575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "grid_s.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'best_score_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgrid_s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_score_\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5569\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   5570\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   5571\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   5572\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5573\u001b[0m ):\n\u001b[0;32m   5574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 5575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'best_score_'"
     ]
    }
   ],
   "source": [
    "grid_s.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 - Use the `os` library to find your cpu count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'cv_results_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m grid_s \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mgrid_s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv_results_\u001b[49m)\n\u001b[0;32m      2\u001b[0m grid_s\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank_test_score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5569\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   5570\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   5571\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   5572\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5573\u001b[0m ):\n\u001b[0;32m   5574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 5575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'cv_results_'"
     ]
    }
   ],
   "source": [
    "grid_s = pd.DataFrame(grid_s.cv_results_)\n",
    "grid_s.sort_values(by=\"rank_test_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 - Create GradientBoostingClassifier base model with set parameters of random state of 77."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3 - Create your param grid. Try avoiding hardcoding the values, if you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.4 - Create the GridSearchCV object and pay attention to it's parameters also besides param_grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.5 - Fit the GridSearch object to our training features and target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.6 - Print the best parameters and the their best score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.7 - Create a dataframe of the fitted results of the Grid Search object. Return the 5 highest ranked cross-validation mean test scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dykOgkEx36pv"
   },
   "source": [
    "# 2.1 Column Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's first create our dataframe that we will use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "8UeaNKWaBXfk"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>voted</th>\n",
       "      <th>city</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>23</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>44</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Valencia</td>\n",
       "      <td>63</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>31</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   voted       city  age   income gender\n",
       "0      1     Madrid   23  20000.0      M\n",
       "1      0     Madrid   44  40000.0      F\n",
       "2      1  Barcelona   88      NaN      F\n",
       "3      0   Valencia   63  35000.0      M\n",
       "4      1              31  30000.0      M"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exercise = pd.DataFrame({\n",
    "    'voted': [1, 0, 1, 0, 1],\n",
    "    'city': ['Madrid', 'Madrid', 'Barcelona', 'Valencia', ''],\n",
    "    'age': [23, 44, 88, 63, 31],\n",
    "    'income': [20000, 40000, None, 35000, 30000],\n",
    "    'gender': ['M', 'F', 'F', 'M', 'M']\n",
    "})\n",
    "\n",
    "df_exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also split it into features and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, target = df_exercise.drop(\"voted\", axis=1), df_exercise[\"voted\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 - Use Pandas to figure out what columns are NA. \n",
    "- What can you spot?\n",
    "- Why is it so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "voted     0\n",
       "city      0\n",
       "age       0\n",
       "income    1\n",
       "gender    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exercise.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 - Create a missing data transformer using ColumnTransformer [Docs](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) and SimpleImputer [Docs](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html). \n",
    "- Fill the `city` empty values with a constant 'Other'.\n",
    "- Fill the `income` NaN's with mean.\n",
    "- Fit transform the dataset using the `features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    city    age  income  gender\n",
       "0  False  False   False   False\n",
       "1  False  False   False   False\n",
       "2  False  False    True   False\n",
       "3  False  False   False   False\n",
       "4  False  False   False   False"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 - Use OneHotEncoder to transform `city` and `gender` columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 - Use StandardScaler to transform `income` and `age` columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Pipelines\n",
    "* Now that we better understand the intrinsics of Column Transformers, let's use `churn` dataset to apply Pipelines on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will create exactly the same model but use pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = churn_df.drop(\"Churn\", axis=1), churn_df[\"Churn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create the train_test_split again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline objects have the following syntax\n",
    "\n",
    "```\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "      ('step_name1', function_to_execute_1()),\n",
    "      ('step_name2', function_to_execute_2()),\n",
    "      ('step_name3', function_to_execute_3())\n",
    "      ]\n",
    "    )\n",
    "```\n",
    "Create a pipeline that has the same steps as in the first model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2.1 - Create pipeline for including a StandardScaler and RandomForestClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2.2 - Fit the pipeline to the training data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2.3 - Define param_grid.\n",
    "* Use a combination of n_estimators of 300, 500, 800 and max_depth of 4, 6, 8.\n",
    "\n",
    "Parameter grids are dictionaries where the parameter we want to search over is the key, and the parameter values are a list of values.\n",
    "\n",
    "```\n",
    "param_grid = {\n",
    "    'stepname__parameter_one':[0, 1, 2, 3, 4],\n",
    "    'stepname__parameter_two': [2, 3, 5, 7, 11, 13]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2.4 - Define a GridSearch Object\n",
    "You can leveage [this](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) documenation. The estimator will be our pipeline and the paramater_grid the parameter grid. There are more optional arguments you can pass in.\n",
    "* Use crossvalidation with 5 folds. \n",
    "* Use accuracy for scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2.5 Fit the Grid Search Model on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2.6 Get the best score and best params."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ZRODuGxTVbF6"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
