{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"200\" style=\"float:left\" \n",
    "     src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sections\n",
    "* [Description](#0)\n",
    "* [1. Setup](#1)\n",
    "  * [1.1 Start Hadoop](#1.1)  \n",
    "  * [1.2 Search for Spark Installation](#1.2)\n",
    "  * [1.3 Create SparkSession](#1.3)\n",
    "* [2. Lab](#2)\n",
    "  * [2.1 Check Files](#2.1)\n",
    "* [3. Dataset Documentation](#3)\n",
    "* [4. DataFrame Creation](#4)\n",
    "* [5. DataFrame Inspection](#5)\n",
    "  * [5.1 Schema Inspection](#5.1)\n",
    "  * [5.2 Content Inspection](#5.2)\n",
    "* [6. DataFrame Transformations](#6)\n",
    "  * [6.1 Column Manipulation](#6.1)\n",
    "  * [6.2 Row Filtering](#6.2)\n",
    "  * [6.3 Row Sorting](#6.2)\n",
    "* [7. TearDown](#7)\n",
    "  * [7.1 Stop Hadoop](#7.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "## Description\n",
    "<p>\n",
    "<div>The goals for this lab are:</div>\n",
    "<ul>    \n",
    "    <li>Get familiar with Spark DataFrames API</li>\n",
    "    <li>Apply some transformations using Spark DataFrames API</li>\n",
    "</ul>    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Setup\n",
    "\n",
    "Since we are going to process data stored from HDFS let's start the service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.1'></a>\n",
    "### 1.1 Start Hadoop\n",
    "\n",
    "Start Hadoop\n",
    "\n",
    "Open a terminal and execute\n",
    "```sh\n",
    "hadoop-start.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.2'></a>\n",
    "### 1.2 Search for Spark Installation \n",
    "This step is required just because we are working in the course environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm changing pandas max column width property to improve data displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.3'></a>\n",
    "### 1.3 Create SparkSession\n",
    "By setting this environment variable we can include extra libraries in our Spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = ' pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing always is to create the SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    ".appName(\"Pokemon - DataFrames - Lab\")\n",
    ".config(\"spark.sql.warehouse.dir\",\"hdfs://localhost:9000/warehouse\")\n",
    ".getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.1'></a>\n",
    "### 2.1 Check  Files\n",
    "\n",
    "In order to complete this lab you need to previosly complete **'Pokemon - RAW to STD - DataFrames'**.<br/>\n",
    "Check you have the data ready in HDFS\n",
    "\n",
    "http://localhost:50070/explorer.html#/datalake/std/pokemon/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Dataset Documentation\n",
    "\n",
    "### Metadata\n",
    "\n",
    "id: ID for each pokemon <br/>\n",
    "name: Name of each pokemon<br/>\n",
    "type_1: Each pokemon has a type, this determines weakness/resistance to attacks<br/>\n",
    "type_2: Some pokemon are dual type and have 2<br/>\n",
    "total: sum of all stats that come after this, a general guide to how strong a pokemon is<br/>\n",
    "hp: hit points, or health, defines how much damage a pokemon can withstand before fainting<br/>\n",
    "attack: the base modifier for normal attacks (eg. Scratch, Punch)<br/>\n",
    "defense: the base damage resistance against normal attacks<br/>\n",
    "sp_atk: special attack, the base modifier for special attacks (e.g. fire blast, bubble beam)<br/>\n",
    "sp_def: the base damage resistance against special attacks<br/>\n",
    "speed: determines which pokemon attacks first each round<br/>\n",
    "generation: pokemon generation<br/>\n",
    "lengendary: determines if the pokemon is legendary or not<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4. DataFrame Creation\n",
    "\n",
    "We can create a DataFrame from extenal sources (like HDFS, databases ...) using the DataSources API, this is usinf the read method in the SparkSession\n",
    "#### read: creates a Spark DataFrame using a Spark DataSource API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"hdfs://localhost:9000/datalake/std/pokemon/pokemon-data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### createDataFrame: creates a Spark DataFrame from a python collection\n",
    "We can also create a DataFrame from a python data structure like lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "data = [Row(0,\"MewThree\",\"Diamond\",\"Energy\", 500, 500, 500,500,500,500,500,0,True)]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\",IntegerType(),True),\n",
    "    StructField(\"name\",StringType(),True),\n",
    "    StructField(\"type_1\",StringType(),True),\n",
    "    StructField(\"type_2\",StringType(),True),\n",
    "    StructField(\"total\",IntegerType(),True),\n",
    "    StructField(\"hp\",IntegerType(),True),\n",
    "    StructField(\"attack\",IntegerType(),True),\n",
    "    StructField(\"defense\",IntegerType(),True),\n",
    "    StructField(\"sp_atk\",IntegerType(),True),\n",
    "    StructField(\"sp_def\",IntegerType(),True),\n",
    "    StructField(\"speed\",IntegerType(),True),\n",
    "    StructField(\"generation\",IntegerType(),True),\n",
    "    StructField(\"legendary\",BooleanType(),True)\n",
    "])\n",
    "\n",
    "df2 = spark.createDataFrame(data, schema)\n",
    "df2.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify the schema using a DDL string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl = \"\"\"\n",
    "            id int,\n",
    "            name string,\n",
    "            type_1 string,\n",
    "            type_2 string,\n",
    "            total int,\n",
    "            hp int,\n",
    "            attack int,\n",
    "            defense int,\n",
    "            sp_atk int,\n",
    "            sp_def int,\n",
    "            speed int,\n",
    "            generation int,\n",
    "            legendary boolean\n",
    "        \"\"\"\n",
    "df3 = spark.createDataFrame(data, ddl)\n",
    "df3.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5. DataFrame Inspection\n",
    "<a id='5.1'></a>\n",
    "### 5.1 Schema Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### schema : Returns a Spark schema object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dtypes : Returns a list of tuples with column names and data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns : Returns a list of column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### printSchema : Prints the DataFrame schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2'></a>\n",
    "### 5.2 Content Inspection\n",
    "Actions are functions that <b>return values,calculations or information</b> about a DataFrame.<br/>\n",
    "Actions <b>are eager<b/> and force DataFrame computation inmediately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### show: Show n first rows formated as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name column values are truncated due to their length <br/>\n",
    "We can enforce Spark to not truncate column values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect : Retrieves all the rows to the driver\n",
    "<span style=\"color:red\">CAUTION: If you collect a DataFrame big enough (millions of rows) your application will blow up</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = df.collect()\n",
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the rows (and it's data) in 'row' python variable.<br/>\n",
    "We can access the values like we were accesing a python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[0][\"name\"]\n",
    "rows[0].name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### toPandas : Retrieves all the rows to the driver and returns a pandas.DataFrame\n",
    "<span style=\"color:red\">CAUTION: If you collect a DataFrame big enough (millions of rows) your application will blow up</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### take : Returns n first rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tail : Returns n last rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### head : Returns the first row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first : Returns the first row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count: Count total number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### describe: Returns a new DataFrame with stats\n",
    "Is a transformation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = df.describe()\n",
    "stats_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### summary: Returns a new DataFrame with stats \n",
    "Is a transformation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = df.summary()\n",
    "summary_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use summary to just compute the stats we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.summary(\"stddev\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## 6. DataFrame Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations are **functions** that can be applied to DataFrames **returning**  **new DataFrames**<br/>\n",
    "Transformations **are lazy** and when applied they are just stacked until an **action** triggers the computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.1'></a>\n",
    "\n",
    "### 6.1 Column Manipulation\n",
    "\n",
    "#### select: Returns a new DataFrame with the column expressions\n",
    "\n",
    "**Flavour 1** select transformation passing a list of column names (**list of strings**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"name\", \"type_1\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select([\"name\", \"type_1\"]).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"*\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flavour 2** select transformation passing a list of Spark columns (**list of Column**):<br/>\n",
    "Spark comes with a bunch of functions out of the box<br/>\n",
    "<a href=\"http://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions\">pyspark.sql.functions</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "type(F.upper(df['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(F.upper(df['name'])).show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a Column expresion with the **expr** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(F.expr(\"upper(name)\")).show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### withColumn: Returns a new DataFrame with the new column expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"new\",F.expr(\"total + hp\")).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### withColumnRenamed: Returns a new DataFrame with renaming a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumnRenamed(\"#\",\"id\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drop: Returns a new DataFrame removing the specified columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"new\",F.expr(\"total + hp\")).drop(\"new\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.2'></a>\n",
    "### 6.2 Row Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter: Returns a new DataFrame with the rows that match the predicate\n",
    "#### where: Alias of filter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df['type_1']=='Fire').show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(\"type_1='Fire'\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter predicates may be as complex as we need to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where((df['type_1']=='Fire') & ((df['hp'] > 100) | (df['defense'] > 100 ))).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(df.name.like('Mew%')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(\"name like 'Mew%'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distinct: Returns a new DataFrame removing duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dropDuplicates: Returns a new DataFrame removing duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### limit: Returns a new DataFrame with a fixed number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.3'></a>\n",
    "### 6.3 Row Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sort: Returns a new DataFrame with the rows sorted\n",
    "\n",
    "#### groupBy: Alias of sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort(F.col(\"name\")).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In desceding order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort(F.col(\"name\").desc()).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "## 7. Tear Down\n",
    "\n",
    "Once we complete the the lab we can stop all the services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7.1'></a>\n",
    "### 7.1 Stop Hadoop\n",
    "\n",
    "Stops Hadoop\n",
    "Open a terminal and execute\n",
    "```sh\n",
    "hadoop-stop.sh\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
