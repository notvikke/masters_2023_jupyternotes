{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"200\" style=\"float:left\" \n",
    "     src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sections\n",
    "* [Description](#0)\n",
    "* [1. Setup](#1)\n",
    "  * [1.1 Start Hadoop](#1.1)  \n",
    "  * [1.2 Search for Spark Installation](#1.2)\n",
    "  * [1.3 Create SparkSession](#1.3)\n",
    "* [2. Practice](#2)\n",
    "* [3. TearDown](#3)\n",
    "  * [3.1 Stop Hadoop](#3.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "## Description\n",
    "<p>\n",
    "<div>The goal for this notebok is getting familiar with the core concepts of Apache Spark:</div>\n",
    "<ul>    \n",
    "    <li>SparkSession</li>\n",
    "    <li>DataFrames</li>\n",
    "    <li>RDD</li>\n",
    "    <li>Partitions</li>\n",
    "    <li>Actions</li>\n",
    "    <li>Transformations</li>\n",
    "</ul>    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Setup\n",
    "\n",
    "Since we are going to process data stored from HDFS let's start the service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.1'></a>\n",
    "### 1.1 Start Hadoop\n",
    "\n",
    "Start Hadoop\n",
    "\n",
    "Open a terminal and execute\n",
    "```sh\n",
    "hadoop-start.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.2'></a>\n",
    "### 1.2 Search for Spark Installation \n",
    "This step is required just because we are working in the course environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm changing pandas max column width property to improve data displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.3'></a>\n",
    "### 1.3 Create SparkSession\n",
    "By setting this environment variable we can include extra libraries in our Spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = ' pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing always is to create the SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    ".appName(\"Pokemon - Data - Lab\")\n",
    ".config(\"spark.sql.warehouse.dir\",\"hdfs://localhost:9000/warehouse\")\n",
    ".getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Practice\n",
    "\n",
    "spark variable is an instance of SparkSession class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can check the type of a variable just using the type function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames is the high level data structure in Apache Spark.\n",
    "\n",
    "We can create DataFrames in two ways. \n",
    "\n",
    "- programmatically\n",
    "- external data source\n",
    "\n",
    "In this case we are going to create a DataFrame by \"reading\" directory in HDFS.\n",
    "\n",
    "One important thing to remember is that we should **ALWAYS keep the same data format** (csv, json, parquet, ...) for all the files inside a folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon_raw = (spark.read                    \n",
    "                    .option(\"header\", \"true\")\n",
    "                    .csv(\"hdfs://localhost:9000/datalake/raw/pokemon/pokemon-data/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pokemon_raw is an instance of DataFrame class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pokemon_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By printing the variable we can see the column names and their types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's check its structure (schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon_raw.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark did not perform any kind of processing up to this point. We are going to call **show** function in the DataFrame, which is an **action**, to ask Spark to show the first 5 elements in the DataFrame. By calling this function, the DataFrame gets triggered and Spark will rewind to the very begining and start to do some work. In this case, the DataFrame has no transformations yet, so is going to ask the first executor to return to the driver the first 5 elements. Once received the driver will print the data in the notebook console\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "**show** is an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon_raw.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that some columns are numerical not a string ðŸ¤”.\n",
    "\n",
    "We can force Spark to infer the data types of every column when reading CSV files. In this case lets use optional property inferSchema.\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "Infering the schema has some penalty in performace since Spark will have to read all the files to actually infer the proper type per column (if there are many files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferSchema is telling Spark to find out the proper data type of each column\n",
    "pokemon_raw = (spark.read\n",
    "              .option(\"header\",\"true\")              \n",
    "              .option(\"inferSchema\",\"true\")\n",
    "              .csv(\"hdfs://localhost:9000/datalake/raw/pokemon/pokemon-data/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now numerical columns are integers.\n",
    "\n",
    "Is very important to **ALWAYS check we have the proper data types** in each columns, as depending on the data type, we would use different funtions. Spark bundles functions for dealing with numbers, string, dates , ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon_raw.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes is usefull to transform a **Spark DataFrame** into a **Pandas DataFrame** for ploting the data or to use another library that relies/depends on Pandas.\n",
    "\n",
    "Transforming a **Spark DataFrame** into a **Pandas DataFrame** is a risky operation, since most of the times, we will be working with datasets that can't fit one single machine memory; and if we transform a big **Spark DataFrame** the driver will fail and the application will stop\n",
    "\n",
    "To take less risks, I'm going to truncate the DataFrame to just 10 elements, and then transform it to Pandas\n",
    "\n",
    "**NOTE:**\n",
    "\n",
    "**limit** is a tranformation\n",
    "\n",
    "**toPandas** is an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon_raw.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High Level API data structures like DataFrames rely internally in the low level API data structure called RDD.\n",
    "\n",
    "We can access the interall RDD via the rdd property in the DataFrame variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pokemon_raw.rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By printing the RDD is not so obvious what holds inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon_raw.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the data, no matter if we are using DataFrames or RDD, is divided in chuncks called **partitions** that are distributed among the available executors.\n",
    "\n",
    "We can get the number of partitions by calling **getNumPartitions** function in an rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon_raw.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD have similar functions to DataFrame ones.\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "**take** is an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon_raw.rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create RDD directly by using the old SparkContext API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon_raw = spark.sparkContext.textFile(\"hdfs://localhost:9000/datalake/raw/pokemon/pokemon-data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pokemon_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, since we created the rdd from a DataFrame, Spark alredy splitted and casted the values to their proper data types.\n",
    "\n",
    "Let's check the contents of an RDD, read from the same directory but this time using the RDD API\n",
    "\n",
    "Well, we have the lines of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon_raw.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To structure the line in columns we need to take care of spliting the columns using the delimiter ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon_raw.map(lambda l: l.split(\",\")).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will still have to cast some columns to integer ...\n",
    "\n",
    "Working with DataFrames makes possible to just focus on the processing and analytics we need to perform instead of worrying about low-level details like having to deal with the file formats for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Tear Down\n",
    "\n",
    "Once we complete the the lab we can stop all the services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.1'></a>\n",
    "### 3.1 Stop Hadoop\n",
    "\n",
    "Stops Hadoop\n",
    "Open a terminal and execute\n",
    "```sh\n",
    "hadoop-stop.sh\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
