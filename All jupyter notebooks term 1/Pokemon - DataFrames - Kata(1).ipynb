{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"200\" style=\"float:left\" \n",
    "     src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sections\n",
    "* [Description](#0)\n",
    "* [1. Setup](#1)\n",
    "  * [1.1 Start Hadoop](#1.1)  \n",
    "  * [1.2 Search for Spark Installation](#1.2)\n",
    "  * [1.3 Create SparkSession](#1.3)\n",
    "* [2. Kata](#2)\n",
    "* [3. TearDown](#3)\n",
    "  * [3.1 Stop Hadoop](#3.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "## Description\n",
    "<p>\n",
    "<div>The goals for this lab are:</div>\n",
    "<ul>    \n",
    "    <li>Get familiar with Spark DataFrames API</li>\n",
    "    <li>Apply some transformations using Spark DataFrames API</li>\n",
    "</ul>    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Setup\n",
    "\n",
    "Since we are going to process data stored from HDFS let's start the service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.1'></a>\n",
    "### 1.1 Start Hadoop\n",
    "\n",
    "Start Hadoop\n",
    "\n",
    "Open a terminal and execute\n",
    "```sh\n",
    "hadoop-start.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.2'></a>\n",
    "### 1.2 Search for Spark Installation \n",
    "This step is required just because we are working in the course environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm changing pandas max column width property to improve data displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.3'></a>\n",
    "### 1.3 Create SparkSession\n",
    "By setting this environment variable we can include extra libraries in our Spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = ' pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing always is to create the SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    ".appName(\"Pokemon - DataFrames - Kata\")\n",
    ".config(\"spark.sql.warehouse.dir\",\"hdfs://localhost:9000/warehouse\")\n",
    ".getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.1'></a>\n",
    "### 2.1 Check  Files\n",
    "\n",
    "In order to complete this lab you need to previosly complete **'Pokemon - RAW to STD - DataFrames'**.<br/>\n",
    "Check you have the data ready in HDFS\n",
    "\n",
    "http://localhost:50070/explorer.html#/datalake/std/pokemon/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dataset Documentation\n",
    "\n",
    "### Metadata\n",
    "\n",
    "id: ID for each pokemon <br/>\n",
    "name: Name of each pokemon<br/>\n",
    "type_1: Each pokemon has a type, this determines weakness/resistance to attacks<br/>\n",
    "type_2: Some pokemon are dual type and have 2<br/>\n",
    "total: sum of all stats that come after this, a general guide to how strong a pokemon is<br/>\n",
    "hp: hit points, or health, defines how much damage a pokemon can withstand before fainting<br/>\n",
    "attack: the base modifier for normal attacks (eg. Scratch, Punch)<br/>\n",
    "defense: the base damage resistance against normal attacks<br/>\n",
    "sp_atk: special attack, the base modifier for special attacks (e.g. fire blast, bubble beam)<br/>\n",
    "sp_def: the base damage resistance against special attacks<br/>\n",
    "speed: determines which pokemon attacks first each round<br/>\n",
    "generation: pokemon generation<br/>\n",
    "legendary: determines if the pokemon is legendary or not<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"hdfs://localhost:9000/datalake/std/pokemon/pokemon-data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.2'></a>\n",
    "### 2.2 Exercises\n",
    "\n",
    "In case you need to use a Spark builtin function: \n",
    "\n",
    "https://spark.apache.org/docs/3.2.1/api/python/reference/pyspark.sql.html#functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all legendary pokemons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all possible pokemon types (type_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find top 10 most powerful pokemons (Total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate pokemon stats for all numerical columns (from Total to Speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a new column called power with the ratio between Total and HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform pokemon names to upper case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all pokemons which names are comprised of more than one word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double the speed of electric pokemons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the DataFrame in JSON format in the following HDFS path hdfs://localhost:9000/datalake/work/pokemon-lab/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the DataFrame back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop generation column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the DataFrame schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the Spark DataFrame into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Tear Down\n",
    "\n",
    "Once we complete the the lab we can stop all the services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.1'></a>\n",
    "### 3.1 Stop Hadoop\n",
    "\n",
    "Stops Hadoop\n",
    "Open a terminal and execute\n",
    "```sh\n",
    "hadoop-stop.sh\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
