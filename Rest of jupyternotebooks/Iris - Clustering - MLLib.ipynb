{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"200\" style=\"float:left\" \n",
    "     src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"display: float:left\" src=\"https://storage.googleapis.com/kaggle-datasets-images/903978/1533070/57da797ac0a3334dfa9e0eda0f5559cc/dataset-cover.jpg?t=2020-10-14-15-50-13\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sections\n",
    "* [Description](#0)\n",
    "* [1. Setup](#1)\n",
    "  * [1.1 Start Hadoop](#1.1)  \n",
    "  * [1.2 Search for Spark Installation](#1.2)\n",
    "  * [1.3 Create SparkSession](#1.3)\n",
    "* [2. Lab](#2)\n",
    "  * [2.1 Check Lab Files](#2.1)\n",
    "* [3. Clustering](#3)\n",
    "* [4. TearDown](#4)\n",
    "  * [4.1 Stop Hadoop](#4.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "## Description\n",
    "<p>\n",
    "In this notebook, we are going to use K-Means to cluster our data. \n",
    "\n",
    "We will be using the Iris dataset, which has labels.\n",
    "    \n",
    "</p>\n",
    "In thi lab we will use Apache Spark to do unsupervised learning.     \n",
    "<div>The goal for this lab are:</div>\n",
    "<ul>    \n",
    "    <li>Practice the Spark ML API</li>\n",
    "    <li>Build a K-Means model</li>\n",
    "</ul>    \n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Setup\n",
    "\n",
    "Since we are going to process data stored from HDFS let's start the service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.1'></a>\n",
    "### 1.1 Start Hadoop\n",
    "\n",
    "Start Hadoop\n",
    "\n",
    "Open a terminal and execute\n",
    "```sh\n",
    "hadoop-start.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.2'></a>\n",
    "### 1.2 Search for Spark Installation \n",
    "This step is required just because we are working in the course environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm changing pandas max column width property to improve data displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.3'></a>\n",
    "### 1.3 Create SparkSession\n",
    "\n",
    "By setting this environment variable we can include extra libraries in our Spark cluster.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = ' pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing always is to create the SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"Iris - Clustering - MLlib\")\n",
    "    .config(\"spark.sql.warehouse.dir\",\"hdfs://localhost:9000/warehouse\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.1'></a>\n",
    "### 2.1 Check Lab Files\n",
    "\n",
    "In order to complete this lab you need to previosly upload the datasets into HDFS.<br/>\n",
    "\n",
    "Check you have the data ready in HDFS\n",
    "\n",
    "http://localhost:50070/explorer.html#/datalake/raw/kaggle/iris/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Clustering\n",
    "\n",
    "Let's create the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisDF = (spark.read.option(\"header\",\"true\")\n",
    "                 .option(\"inferSchema\",\"true\")\n",
    "                 .csv(\"hdfs://localhost:9000/datalake/raw/kaggle/iris/\")\n",
    "                 .cache())\n",
    "\n",
    "print(f\"There are {irisDF.count()} rows in the datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisDF.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have four variables we will consider as \"features\".  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"SepalLengthCm\", \"SepalWidthCm\",\"PetalLengthCm\",\"PetalWidthCm\"], outputCol=\"features\")\n",
    "irisFeaturesDF = assembler.transform(irisDF)\n",
    "irisFeaturesDF.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to create another vector of just two feature just for the sake of plotting later the clusters by suing PCA (Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "irisTwoFeaturesDF = pca.fit(irisFeaturesDF).transform(irisFeaturesDF)\n",
    "irisTwoFeaturesDF.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisTwoFeaturesDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Determine the Optimal K for K-Means? \n",
    "\n",
    "In K-means clustering algorithm the number of clusters (k) is the hyper-parameter to be tuned.\n",
    "\n",
    "There are two methods for finding the optimal value for K\n",
    "\n",
    "**1.Silhouette Method:**\n",
    "\n",
    "Based on the metric Silhouett Score: The higher the silhouette score the better is the clustering.\n",
    "\n",
    "The range of the Silhouette value is between +1 and -1. \n",
    "\n",
    "A high value is desirable and indicates that the point is placed in the correct cluster. \n",
    "\n",
    "If many points have a negative Silhouette value, it may indicate that we have created too many or too few clusters.\n",
    "\n",
    "**2.Elbow Method:** \n",
    "\n",
    "Based on the metric WSSSE (Within Set Sum of Squared Errors). The lower the WSSSE the better is the clustering.\n",
    "\n",
    "As the number of clusters increases, the WSSSE value will start to decrease.\n",
    "\n",
    "When we analyze the graph we can see that the graph will rapidly change at a point and thus creating an elbow shape. \n",
    "\n",
    "From this point, the graph starts to move almost parallel to the X-axis. \n",
    "\n",
    "The K value corresponding to this point is the optimal K value or an optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "seed=1234\n",
    "\n",
    "# k's we are going to try: from 2 up to 10 clusters\n",
    "ks = range(2,10)\n",
    "\n",
    "#metrics\n",
    "models=[]\n",
    "wssse=[]\n",
    "sil=[]\n",
    "\n",
    "for k in ks:\n",
    "    kmeans = KMeans(k=k, seed=seed, maxIter=300, featuresCol=\"features\")\n",
    "    model = kmeans.fit(irisTwoFeaturesDF)    \n",
    "    predictions = model.transform(irisTwoFeaturesDF)\n",
    "    models.append(model)\n",
    "    wssse.append(model.summary.trainingCost)\n",
    "    sil.append(evaluator.evaluate(predictions))\n",
    "\n",
    "#silhouette plot\n",
    "plt.plot(range(2,10),sil)\n",
    "plt.title(\"Silhouette Method\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.xlabel(\"Silhouette Score\")\n",
    "plt.show()\n",
    "\n",
    "#elbow plot\n",
    "plt.plot(range(2,10),wssse)\n",
    "plt.title(\"Elbow Method\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.xlabel(\"WSSSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the charts, the optimal value for k would be K=5\n",
    "\n",
    "Lets' visualize the clusters using this time the **pca_features** (to get two dimension centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "kmeans = KMeans(k=5, seed=seed, maxIter=300, featuresCol=\"pca_features\")\n",
    "bestModel = kmeans.fit(irisTwoFeaturesDF)    \n",
    "predictions = bestModel.transform(irisTwoFeaturesDF)\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(f\"Silhouette with squared euclidean distance = {silhouette}\")\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors.\n",
    "wssse = model.summary.trainingCost\n",
    "print(f\"Within Set Sum of Squared Errors = {wssse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = predictions.select((vector_to_array(col('pca_features'))[0]).alias('x'),\n",
    "                        (vector_to_array(col('pca_features'))[1]).alias('y'),\n",
    "                         col('prediction').alias('label')).toPandas()\n",
    "\n",
    "clusters = df['label'].unique()\n",
    "\n",
    "centroids = bestModel.clusterCenters()\n",
    "  \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for i in list(clusters):\n",
    "    t = df.loc[df['label']==i]\n",
    "    ax.scatter(x=t['x'],y=t['y'],label=i)\n",
    "\n",
    "for c in centroids:    \n",
    "    ax.scatter(x=c[0],y=c[1],c='black')\n",
    "       \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4. Tear Down\n",
    "\n",
    "Once we complete the the lab we can stop all the services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.1'></a>\n",
    "### 4.1 Stop Hadoop\n",
    "\n",
    "Stops Hadoop\n",
    "Open a terminal and execute\n",
    "```sh\n",
    "hadoop-stop.sh\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
