{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"200\" style=\"float:left\" \n",
    "     src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:left;height:200\" \n",
    "     src=\"https://storage.googleapis.com/kaggle-datasets-images/1392/2506/2d89d2ffd3946c8e06d9d57a8ffb01ec/dataset-cover.jpg\" />   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sections\n",
    "* [Description](#0)\n",
    "* [1. Setup](#1)\n",
    "  * [1.1 Start Hadoop](#1.1)  \n",
    "  * [1.2 Search for Spark Installation](#1.2)\n",
    "  * [1.3 Create SparkSession](#1.3)\n",
    "* [2. Kata](#2)\n",
    "  * [2.1 Upload the Dataset to HDFS](#2.1)  \n",
    "  * [2.2 Create the DataFrames](#2.2)\n",
    "  * [2.3 Create the GraphFrame](#2.3)\n",
    "  * [2.4 Exercises](#2.4)\n",
    "* [3. TearDown](#3)\n",
    "  * [3.1 Stop Hadoop](#3.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "## Description\n",
    "<p>\n",
    "<div>The goal for this kata are:</div>\n",
    "<ul>    \n",
    "    <li>Practice the Spark GraphFrames API</li>\n",
    "    <li>Solve several exercise by yourself</li>\n",
    "</ul>    \n",
    "</p>\n",
    "\n",
    "<p>We are going to work with a flights dataset. We're now interested in understanding better how the different cities are interconnected together, and which airports are the most important.\n",
    "\n",
    "It happens that airports and flights between them can be modeled as a graph where:\n",
    "\n",
    "- **vertices**: airports.\n",
    "- **edges**: flights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Setup\n",
    "\n",
    "Since we are going to process data stored from HDFS let's start the service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.1'></a>\n",
    "### 1.1 Start Hadoop\n",
    "\n",
    "Start Hadoop\n",
    "\n",
    "Open a terminal and execute\n",
    "```sh\n",
    "hadoop-start.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.2'></a>\n",
    "### 1.2 Search for Spark Installation \n",
    "This step is required just because we are working in the course environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm changing pandas max column width property to improve data displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.3'></a>\n",
    "### 1.3 Create SparkSession\n",
    "\n",
    "By setting this environment variable we can include extra libraries in our Spark cluster.<br/>\n",
    "GraphFrames is not in spark core so we have to add it this way## 2. Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\" --jars /opt/hive3/lib/hive-hcatalog-core-3.1.2.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing always is to create the SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"Flights - Analytics - GraphFrames Kata\")\n",
    "    .config(\"spark.sql.warehouse.dir\",\"hdfs://localhost:9000/warehouse\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Kata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.1'></a>\n",
    "### 2.1 Upload the Dataset to HDFS\n",
    "\n",
    "Upload the dataset provided in the following HDFS path:<br/>\n",
    "/datalake/raw/flights/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.2'></a>\n",
    "### 2.2 Create the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create flights dataframe\n",
    "flights = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#create vertices dataframe with just one column based on all distinct Origin airports.\n",
    "vertices = \n",
    "\n",
    "#create edges dataframe \n",
    "edges = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.3'></a>\n",
    "### 2.3 Create the GraphFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import GraphFrame\n",
    "\n",
    "#create the graphframe\n",
    "graph = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.4'></a>\n",
    "### 2.4 Exercises\n",
    "\n",
    "1. Find out the top 5 airports with the highest number of outbound flights.\n",
    "2. Find out the top 5 airports with the highest number of inbound flights.\n",
    "3. Find out the top 5 most important airports.\n",
    "4. Find out the shortest paths between Albuquerque International Sunport airport and Nashville International Airport.\n",
    "5. Identify routes between airports with no direct connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Find out the top 5 airports with the highest number of outbound flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Find out the top 5 airports with the highest number of inbound flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Find out the top 5 most important airports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Find out the shortest paths between Albuquerque International Sunport airport and Nashville International Airport.\n",
    "\n",
    "Albuquerque International Sunport airport's code is **ABQ** while the Nashville International Airport's one is **BNA**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <b>NOTE</b>: Spark's BFS (Breadth-first search) computes the shortest paths in terms of the <b>number of hops</b> between two vertices. It does <b>not</b> take edge weights into account. There are alternatives to take edge weights into consideration but, unfortunately, it's out of the scope of this course.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Identify routes between airports with no direct connection.\n",
    "**Hint:** Try to find vertices **a**, **b** and **c** where:\n",
    "\n",
    "* There is an **edge from a to b**, an **edge from b to c**, but **no edge from a to c**.\n",
    "* Additionally, you need to ensure that **a and c are not the same vertex**.\n",
    "\n",
    "The easiest way to provide this logic is by **using motif findings**. Given that we haven't spent very much time on this topic, **feel free to skip it if you are not interested in exploring this further**. If you are excited enough to go down this road, check the [Motif Finding section](https://graphframes.github.io/graphframes/docs/_site/user-guide.html#motif-finding) in the [GraphFrames User Guide](https://graphframes.github.io/graphframes/docs/_site/user-guide.html#graphframes-user-guide)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Tear Down\n",
    "\n",
    "Once we complete the the lab we can stop all the services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.1'></a>\n",
    "### 3.1 Stop Hadoop\n",
    "\n",
    "Stops Hadoop\n",
    "Open a terminal and execute\n",
    "```sh\n",
    "hadoop-stop.sh\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
